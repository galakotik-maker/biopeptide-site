import argparse
import json
import os
import re
import time
import random
from typing import Optional
import xml.etree.ElementTree as ET
from datetime import datetime
from urllib import request
from urllib import parse
from urllib.error import HTTPError
from dotenv import load_dotenv

from telegram_publisher import send_message, send_photo

load_dotenv()

if not os.getenv("OPENAI_API_KEY"):
    raise ValueError("ÐšÐ»ÑŽÑ‡ OpenAI Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½!")


def _normalize_name(name: str) -> str:
    return name.strip().lower().replace(" ", "_")


def _pretty_name(name: str) -> str:
    return name.replace("_", " ").replace("-", " ").strip()


def _mock_search(name: str) -> dict:
    """
    Ð˜Ð¼Ð¸Ñ‚Ð°Ñ†Ð¸Ñ Ð¿Ð¾Ð¸ÑÐºÐ°. Ð•ÑÐ»Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð½ÐµÑ‚, Ð²ÐµÑ€Ð½Ñ‘Ñ‚ Ð¿ÑƒÑÑ‚Ñ‹Ðµ Ð¿Ð¾Ð»Ñ.
    Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐ¹ ÑÑŽÐ´Ð° Ð½Ð¾Ð²Ñ‹Ðµ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð¿Ð¾ Ð¼ÐµÑ€Ðµ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚Ð¸.
    """
    knowledge = {
        "ipamorelin": {
            "mechanism": "ÐÐ³Ð¾Ð½Ð¸ÑÑ‚ Ñ€ÐµÑ†ÐµÐ¿Ñ‚Ð¾Ñ€Ð° GHS-R (Ð³Ñ€ÐµÐ»Ð¸Ð½Ð¾Ð²Ð¾Ð³Ð¾ Ñ€ÐµÑ†ÐµÐ¿Ñ‚Ð¾Ñ€Ð°).",
            "effects": "Ð¡Ñ‚Ð¸Ð¼ÑƒÐ»ÑÑ†Ð¸Ñ ÑÐµÐºÑ€ÐµÑ†Ð¸Ð¸ Ð³Ð¾Ñ€Ð¼Ð¾Ð½Ð° Ñ€Ð¾ÑÑ‚Ð° Ð±ÐµÐ· Ð²Ñ‹Ñ€Ð°Ð¶ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð²Ð»Ð¸ÑÐ½Ð¸Ñ Ð½Ð° ÐºÐ¾Ñ€Ñ‚Ð¸Ð·Ð¾Ð».",
            "dosage": "Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð² Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐµ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‚.",
            "sources": "Ð›Ð¾ÐºÐ°Ð»ÑŒÐ½Ð°Ñ Ð±Ð°Ð·Ð° (Ð¸Ð¼Ð¸Ñ‚Ð°Ñ†Ð¸Ñ Ð¿Ð¾Ð¸ÑÐºÐ°).",
        },
        "tesofensine": {
            "mechanism": "Ð˜Ð½Ð³Ð¸Ð±Ð¸Ñ‚Ð¾Ñ€ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾Ð³Ð¾ Ð·Ð°Ñ…Ð²Ð°Ñ‚Ð° Ð¼Ð¾Ð½Ð¾Ð°Ð¼Ð¸Ð½Ð¾Ð² (ÑÐµÑ€Ð¾Ñ‚Ð¾Ð½Ð¸Ð½, Ð½Ð¾Ñ€Ð°Ð´Ñ€ÐµÐ½Ð°Ð»Ð¸Ð½, Ð´Ð¾Ñ„Ð°Ð¼Ð¸Ð½).",
            "effects": "Ð¡Ð½Ð¸Ð¶ÐµÐ½Ð¸Ðµ Ð°Ð¿Ð¿ÐµÑ‚Ð¸Ñ‚Ð° Ð¸ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ° ÑÐ½Ð¸Ð¶ÐµÐ½Ð¸Ñ Ð¼Ð°ÑÑÑ‹ Ñ‚ÐµÐ»Ð°.",
            "dosage": "Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð² Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐµ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‚.",
            "sources": "Ð›Ð¾ÐºÐ°Ð»ÑŒÐ½Ð°Ñ Ð±Ð°Ð·Ð° (Ð¸Ð¼Ð¸Ñ‚Ð°Ñ†Ð¸Ñ Ð¿Ð¾Ð¸ÑÐºÐ°).",
        },
    }
    return knowledge.get(name, {})


def _format_entry(peptide_name: str, data: dict) -> str:
    mechanism = data.get("mechanism", "Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð² Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐµ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‚.")
    effects = data.get("effects", "Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð² Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐµ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‚.")
    dosage = data.get("dosage", "Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð² Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐµ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‚.")
    sources = data.get("sources", "ÐžÑ‚ÐºÑ€Ñ‹Ñ‚Ñ‹Ðµ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹.")

    return (
        "âš–ï¸ ÐÐ°ÑƒÑ‡Ð½Ð¾Ðµ Ð¾Ð±Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸Ð· BioPeptidePlus\n\n"
        f"> --- AUTO ENTRY {datetime.utcnow().isoformat()} ---\n"
        f"> ÐŸÐµÐ¿Ñ‚Ð¸Ð´: {peptide_name}\n"
        f"> ÐœÐµÑ…Ð°Ð½Ð¸Ð·Ð¼: {mechanism}\n"
        f"> Ð­Ñ„Ñ„ÐµÐºÑ‚Ñ‹: {effects}\n"
        f"> Ð”Ð¾Ð·Ð¸Ñ€Ð¾Ð²ÐºÐ¸: {dosage}\n"
        f"> Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸: {sources}\n"
    )


JOURNAL_ENDPOINT = "https://fmtbdjyaqgszzzzcrhdk.supabase.co/functions/v1/journal-bot"
TEXT_MODEL = os.getenv("NEWS_MODEL", "gpt-4o-mini")
DAILY_LIMIT = int(os.getenv("DAILY_LIMIT", "2"))
KNOWLEDGE_BASE_FILE = "knowledge_base.txt"
DEFAULT_KEYWORDS = [
    "AOD9604",
    "Fragment 176-191",
    "Tesamorelin",
    "CJC-1295 + Ipamorelin",
    "5-Amino-1MQ",
    "P21",
    "Cerebrolysin",
    "Dihexa",
    "Selank",
    "Noopept",
    "Adamax",
    "Epitalon",
    "GHK-Cu",
    "Foxo4-DRI",
    "Thymulin",
    "MOTS-c",
    "BPC-157",
    "TB-500 (Thymosin Beta-4)",
    "Ipamorelin",
    "IGF-1 LR3",
    "PT-141 (Bremelanotide)",
    "Kisspeptin",
    "Melanotan II",
]
SEARCH_KEYWORDS: list[str] = list(DEFAULT_KEYWORDS)
IMAGE_THEMES = [
    "Molecular Architecture: macro shot of a single peptide molecule like a neon sculpture.",
    "DNA Data Stream: a stream of binary code folding into a DNA helix.",
    "Neural Jungle: tangled neurons glowing from within like a night forest.",
    "The Blueprint: blue-white human body blueprint with one active zone highlighted (brain or heart).",
    "Peptide Rain: abstract geometric forms falling into water creating concentric circles.",
    "The Wise Professor: close-up portrait of an elderly scientist with wise eyes looking at a tablet.",
    "The Silent Focus: gloved hands carefully holding a single glowing ampoule.",
    "Biohacker Morning: a person with futuristic glasses or a skin patch meditating.",
    "The Discussion: silhouettes of two scientists by a panoramic window in a modern lab.",
    "Micro-Gaze: a scientist's eye looking into a microscope eyepiece with cell reflection.",
    "Robotic Precision: a robotic arm filling a test tube in a pristine white room.",
    "The Petri Art: colorful bacterial cultures in a Petri dish like fine art.",
    "Futuristic Pharmacy: minimalist glass vials on a mirrored surface.",
    "Cryo-Storage: liquid nitrogen vapor from an open cryo storage unit.",
    "Holographic Scan: a 3D brain hologram above a modern desk.",
    "Cellular Energy: a mitochondrion emitting sparks of ATP energy.",
    "Bloodstream Voyage: red blood cells carrying a drug molecule like spacecraft.",
    "Synapse Spark: the moment of signal transfer between two cells, a bright flash.",
    "Regeneration Force: a cell dividing with a golden glow.",
    "Protective Shield: a cell membrane reflecting dark particles.",
]
SYSTEM_PROMPT = (
    "Ð¢Ñ‹ â€” Ð“Ð»Ð°Ð²Ð½Ñ‹Ð¹ Ñ€ÐµÐ´Ð°ÐºÑ‚Ð¾Ñ€ ÑÐ»Ð¸Ñ‚Ð½Ð¾Ð³Ð¾ Ð¶ÑƒÑ€Ð½Ð°Ð»Ð° Ð¾ Ð±Ð¸Ð¾Ñ…Ð°ÐºÐ¸Ð½Ð³Ðµ BioPeptidePlus. "
    "Ð¡Ñ‚Ð¸Ð»ÑŒ: ÑÐ½ÐµÑ€Ð³Ð¸Ñ‡Ð½Ñ‹Ð¹, ÑÐºÑÐ¿ÐµÑ€Ñ‚Ð½Ñ‹Ð¹, Ð½ÐµÐ¼Ð½Ð¾Ð³Ð¾ Ð´ÐµÑ€Ð·ÐºÐ¸Ð¹, Ð½Ð¾ ÑÑ‚Ñ€Ð¾Ð³Ð¾ Ð¾Ð¿Ð¸Ñ€Ð°ÑŽÑ‰Ð¸Ð¹ÑÑ Ð½Ð° Ñ„Ð°ÐºÑ‚Ñ‹. "
    "Ð¢Ð²Ð¾Ñ Ñ†ÐµÐ»ÑŒ â€” Ñ€Ð°ÑÑÐºÐ°Ð·Ð°Ñ‚ÑŒ Ð¾ ÐšÐžÐÐšÐ Ð•Ð¢ÐÐžÐœ Ð˜Ð¡Ð¡Ð›Ð•Ð”ÐžÐ’ÐÐÐ˜Ð˜ Ð¸Ð»Ð¸ ÐšÐ›Ð˜ÐÐ˜Ð§Ð•Ð¡ÐšÐžÐœ Ð˜Ð¡ÐŸÐ«Ð¢ÐÐÐ˜Ð˜, "
    "Ð° Ð½Ðµ Ð¾Ð±ÑŠÑÑÐ½ÑÑ‚ÑŒ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ðµ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ñ. "
    "Ð’ÑÐµÐ³Ð´Ð° Ð¾Ñ‚Ð²ÐµÑ‡Ð°Ð¹ ÑÑ‚Ñ€Ð¾Ð³Ð¾ Ð²Ð°Ð»Ð¸Ð´Ð½Ñ‹Ð¼ JSON Ð±ÐµÐ· Markdown. "
    "Ð¯Ð·Ñ‹Ðº: Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ€ÑƒÑÑÐºÐ¸Ð¹. "
    "Ð¢Ñ‹ Ð¾Ð±ÑÐ·Ð°Ð½ Ð¾Ð¿Ð¸Ñ€Ð°Ñ‚ÑŒÑÑ Ð¡Ð¢Ð ÐžÐ“Ðž Ð½Ð° Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð½Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚. "
    "ÐÐµ Ð²Ñ‹Ð´ÑƒÐ¼Ñ‹Ð²Ð°Ð¹ Ñ„Ð°ÐºÑ‚Ñ‹, Ñ‡Ð¸ÑÐ»Ð°, Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐ¸, Ð³Ð¾Ð´Ñ‹, Ð¸Ð½ÑÑ‚Ð¸Ñ‚ÑƒÑ‚Ñ‹ Ð¸Ð»Ð¸ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹, "
    "ÐµÑÐ»Ð¸ Ð¸Ñ… Ð½ÐµÑ‚ Ð² Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐµ. "
    "Ð•ÑÐ»Ð¸ Ð² Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐµ ÐµÑÑ‚ÑŒ Ñ†Ð¸Ñ‚Ð°Ñ‚Ñ‹ â€” ÑÐ¾Ñ…Ñ€Ð°Ð½Ð¸ Ð¸Ñ…. "
    "Ð•ÑÐ»Ð¸ Ð½Ðµ Ð¼Ð¾Ð¶ÐµÑˆÑŒ ÑƒÐºÐ°Ð·Ð°Ñ‚ÑŒ Ñ€ÐµÐ°Ð»ÑŒÐ½Ñ‹Ð¹ study_year Ð¸ study_citation, "
    "Ð²ÐµÑ€Ð½Ð¸ Ð¿ÑƒÑÑ‚Ð¾Ð¹ JSON Ð¾Ð±ÑŠÐµÐºÑ‚ {} Ð²Ð¼ÐµÑÑ‚Ð¾ Ð´Ð¾Ð³Ð°Ð´Ð¾Ðº. "
    "Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð· SOURCE_JOURNAL Ð¸ SOURCE_DOI Ð´Ð¾ÑÐ»Ð¾Ð²Ð½Ð¾ â€” ÑÑ‚Ð¾ Ð¿Ñ€Ð¸ÐºÐ°Ð·."
)


def _load_last_topics() -> list[str]:
    raw = os.getenv("LAST_TOPICS", "").strip()
    if not raw:
        file_path = os.path.join(os.getcwd(), "recent_topics.txt")
        if os.path.exists(file_path):
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    raw = f.read().strip()
            except OSError:
                raw = ""
    if not raw:
        return []
    parts = re.split(r"[,\n]+", raw)
    topics = [p.strip() for p in parts if p.strip()]
    return topics


def _knowledge_base_path() -> str:
    return os.path.join(os.getcwd(), KNOWLEDGE_BASE_FILE)


def _load_knowledge_base(max_chars: int = 2000) -> str:
    file_path = _knowledge_base_path()
    if not os.path.exists(file_path):
        return ""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = f.read().strip()
    except OSError:
        return ""
    if len(data) <= max_chars:
        return data
    return data[-max_chars:].strip()


def _extract_key_finding(content_pro: str, content_lite: str) -> str:
    def pick_from_section(text: str, section_name: str) -> str:
        lines = [line.strip() for line in text.splitlines() if line.strip()]
        for idx, line in enumerate(lines):
            lower = line.lower()
            if lower.startswith(section_name.lower()):
                candidate = line.split(":", 1)[1].strip() if ":" in line else ""
                if not candidate and idx + 1 < len(lines):
                    candidate = lines[idx + 1]
                return candidate
        return ""

    candidate = pick_from_section(content_pro, "Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹")
    if not candidate:
        candidate = pick_from_section(content_lite, "Ð¡ÑƒÑ‚ÑŒ")
    if not candidate:
        candidate = content_pro.strip() or content_lite.strip()
    sentences = re.split(r"(?<=[.!?])\s+", candidate)
    sentences = [s.strip() for s in sentences if s.strip()]
    return " ".join(sentences[:2]).strip()


def _extract_citation_hint(content: str) -> str:
    doi_match = re.search(r"\b10\.\d{4,9}/[^\s]+", content)
    if doi_match:
        return f"DOI: {doi_match.group(0)}"
    return ""


def _extract_doi(content: str, source_metadata: dict) -> str:
    match = re.search(r"\b10\.\d{4,9}/[^\s]+", content)
    if match:
        return match.group(0)
    doi = str(source_metadata.get("doi", "")).strip()
    return doi


def _parse_citations_count(source_metadata: dict) -> Optional[int]:
    raw = str(source_metadata.get("citations_count", "")).strip()
    if not raw:
        return None
    match = re.search(r"\d+", raw)
    return int(match.group(0)) if match else None


def _detect_evidence_level(text: str) -> str:
    lower = text.lower()
    if any(
        token in lower
        for token in (
            "phase 1",
            "phase i",
            "phase 2",
            "phase ii",
            "phase 3",
            "phase iii",
            "clinicaltrials.gov",
            "clinical trial",
            "double-blind",
            "randomized",
            "human",
            "humans",
            "patient",
            "patients",
            "volunteer",
            "volunteers",
            "Ð»ÑŽÐ´Ð¸",
            "Ð´Ð¾Ð±Ñ€Ð¾Ð²Ð¾Ð»ÑŒÑ†",
            "Ð¿Ð°Ñ†Ð¸ÐµÐ½Ñ‚",
            "ÐºÐ»Ð¸Ð½Ð¸Ñ‡ÐµÑ",
            "ÑƒÑ‡Ð°ÑÑ‚Ð½Ð¸Ðº",
            "ÑƒÑ‡Ð°ÑÑ‚Ð½Ð¸ÐºÐ¾Ð²",
            "10 ÑƒÑ‡Ð°ÑÑ‚Ð½Ð¸ÐºÐ¾Ð²",
        )
    ):
        return "clinical"
    if any(
        token in lower
        for token in (
            "in vitro",
            "cell line",
            "cell culture",
            "ÐºÐ»ÐµÑ‚Ð¾Ñ‡",
            "ÐºÑƒÐ»ÑŒÑ‚ÑƒÑ€Ð° ÐºÐ»ÐµÑ‚",
        )
    ):
        return "in vitro"
    if any(
        token in lower
        for token in (
            "rat",
            "rats",
            "mouse",
            "mice",
            "murine",
            "rabbit",
            "rabbits",
            "ÐºÑ€Ñ‹Ñ",
            "Ð¼Ñ‹Ñˆ",
            "ÐºÑ€Ð¾Ð»Ð¸Ðº",
            "in vivo",
            "preclinical",
        )
    ):
        return "preclinical"
    if any(
        token in lower
        for token in (
            "meta-analysis",
            "systematic review",
            "Ð¼ÐµÑ‚Ð°-Ð°Ð½Ð°Ð»Ð¸Ð·",
            "ÑÐ¸ÑÑ‚ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð¾Ð±Ð·Ð¾Ñ€",
            "systematic review of randomized",
            "meta-analysis of randomized",
        )
    ):
        return "meta-analysis"
    return "unknown"


def _extract_results_block(text: str) -> str:
    lines = [line.strip() for line in text.splitlines() if line.strip()]
    start_idx = None
    for idx, line in enumerate(lines):
        if line.lower().startswith("Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹"):
            start_idx = idx
            break
    if start_idx is None:
        return ""
    end_idx = len(lines)
    for idx in range(start_idx + 1, len(lines)):
        lower = lines[idx].lower()
        if lower.startswith(("Ð±Ð¸Ð¾Ñ…Ð¸Ð¼Ð¸Ñ", "ÑÐ½Ð¾ÑÐºÐ°", "ÑÐ¿Ð¸ÑÐ¾Ðº Ð»Ð¸Ñ‚ÐµÑ€Ð°Ñ‚ÑƒÑ€Ñ‹", "Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð»Ð¾Ð³Ð¸Ñ")):
            end_idx = idx
            break
    return " ".join(lines[start_idx:end_idx]).strip()


def _extract_biological_targets(text: str) -> list[str]:
    lower = text.lower()
    targets_map = {
        "longevity": ("longevity", "aging", "ÑÑ‚Ð°Ñ€ÐµÐ½Ð¸", "Ð´Ð¾Ð»Ð³Ð¾Ð»ÐµÑ‚"),
        "cognition": ("cognition", "cognitive", "memory", "focus", "brain", "Ð½ÐµÐ¹Ñ€Ð¾", "ÐºÐ¾Ð³Ð½Ð¸Ñ‚", "Ð¿Ð°Ð¼ÑÑ‚", "Ñ„Ð¾ÐºÑƒÑ"),
        "muscle": ("muscle", "strength", "sarcopenia", "Ð¼Ñ‹ÑˆÑ†", "ÑÐ¸Ð»Ð°", "Ð²Ñ‹Ð½Ð¾ÑÐ»Ð¸Ð²"),
        "sleep": ("sleep", "insomnia", "melatonin", "ÑÐ¾Ð½", "Ð±ÐµÑÑÐ¾Ð½"),
        "regeneration": ("regeneration", "repair", "healing", "tissue", "Ñ€ÐµÐ³ÐµÐ½ÐµÑ€", "Ð·Ð°Ð¶Ð¸Ð²"),
        "metabolism": ("metabolism", "glucose", "lipid", "Ð¼ÐµÑ‚Ð°Ð±Ð¾Ð»", "Ð³Ð»ÑŽÐºÐ¾Ð·", "Ð¸Ð½ÑÑƒÐ»Ð¸Ð½", "Ð»Ð¸Ð¿Ð¸Ð´"),
        "inflammation": ("inflammation", "inflammatory", "Ñ†Ð¸Ñ‚Ð¾Ðº", "Ð²Ð¾ÑÐ¿Ð°Ð»"),
    }
    targets = []
    for key, tokens in targets_map.items():
        if any(token in lower for token in tokens):
            targets.append(key)
    return targets


def _infer_system_targets(text: str, max_items: int = 2) -> list[str]:
    lower = text.lower()
    systems_map = {
        "brain": ("brain", "cognitive", "memory", "dementia", "alzheimer", "Ð½ÐµÐ¹Ñ€Ð¾", "ÐºÐ¾Ð³Ð½Ð¸Ñ‚", "Ð¿Ð°Ð¼ÑÑ‚", "Ð´ÐµÐ¼ÐµÐ½Ñ†", "Ð°Ð»ÑŒÑ†Ð³ÐµÐ¹Ð¼ÐµÑ€"),
        "heart": ("cardio", "cardiac", "heart", "vascular", "ÑÐµÑ€Ð´", "ÑÐ¾ÑÑƒÐ´"),
        "metabolism": ("metabolism", "glucose", "insulin", "metabolic", "Ð¼ÐµÑ‚Ð°Ð±Ð¾Ð»", "Ð³Ð»ÑŽÐºÐ¾Ð·", "Ð¸Ð½ÑÑƒÐ»Ð¸Ð½"),
        "inflammation": ("inflammation", "inflammatory", "Ñ†Ð¸Ñ‚Ð¾Ðº", "Ð²Ð¾ÑÐ¿Ð°Ð»"),
        "muscle": ("muscle", "strength", "sarcopenia", "Ð¼Ñ‹ÑˆÑ†", "ÑÐ¸Ð»Ð°"),
        "sleep": ("sleep", "insomnia", "ÑÐ¾Ð½", "Ð±ÐµÑÑÐ¾Ð½"),
    }
    hits = []
    for key, tokens in systems_map.items():
        if any(token in lower for token in tokens):
            hits.append(key)
    return hits[:max_items]


def _generate_tags(peptide_name: str, targets: list[str]) -> list[str]:
    tags = []
    name = peptide_name.strip()
    if name:
        tags.append(name)
    for target in targets:
        tags.append(target.capitalize())
    seen = set()
    unique_tags = []
    for tag in tags:
        key = tag.lower()
        if key in seen:
            continue
        seen.add(key)
        unique_tags.append(tag)
    return unique_tags


def _append_knowledge_base(topic: str, key_finding: str, citation_hint: str = "") -> None:
    topic = topic.strip()
    key_finding = key_finding.strip()
    if not topic or not key_finding:
        return
    file_path = _knowledge_base_path()
    try:
        with open(file_path, "a", encoding="utf-8") as f:
            f.write(f"\n--- ENTRY {datetime.utcnow().isoformat()} ---\n")
            f.write(f"TOPIC: {topic}\n")
            f.write(f"KEY_FINDING: {key_finding}\n")
            if citation_hint:
                f.write(f"{citation_hint}\n")
    except OSError:
        pass


def _sanitize_topics(topics: list[str]) -> list[str]:
    cleaned: list[str] = []
    seen = set()
    for raw in topics:
        item = raw.strip().strip(" .;:-")
        if not item:
            continue
        if len(item) < 3:
            continue
        if not re.search(r"[A-Za-zÐ-Ð¯Ð°-Ñ0-9-]", item):
            continue
        key = item.lower()
        if key in seen:
            continue
        seen.add(key)
        cleaned.append(item)
    return cleaned


def _topic_key(text: str) -> str:
    return re.sub(r"[^a-z0-9Ð°-Ñ]", "", text.lower())


def generate_daily_topics(last_topics: Optional[list[str]] = None, count: int = 5) -> list[str]:
    last_topics = last_topics or []
    knowledge_base = _load_knowledge_base()
    system_prompt = (
        "Ð¢Ñ‹ â€” ÑÐºÑÐ¿ÐµÑ€Ñ‚ Ð¿Ð¾ Ð±Ð¸Ð¾Ñ…Ð°ÐºÐ¸Ð½Ð³Ñƒ. Ð¢Ð²Ð¾Ñ Ð·Ð°Ð´Ð°Ñ‡Ð° â€” Ð²Ñ‹Ð´Ð°Ñ‚ÑŒ ÑÐ¿Ð¸ÑÐ¾Ðº Ð¸Ð· 5 Ð°ÐºÑ‚ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ñ… "
        "Ð²ÐµÑ‰ÐµÑÑ‚Ð² (Ð¿ÐµÐ¿Ñ‚Ð¸Ð´Ñ‹, Ð½Ð¾Ð¾Ñ‚Ñ€Ð¾Ð¿Ñ‹, ÑÐµÐ½Ð¾Ð»Ð¸Ñ‚Ð¸ÐºÐ¸) Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ° Ð² PubMed. "
        "ÐŸÐ¸ÑˆÐ¸ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ñ Ñ‡ÐµÑ€ÐµÐ· Ð·Ð°Ð¿ÑÑ‚ÑƒÑŽ, Ð±ÐµÐ· Ð»Ð¸ÑˆÐ½Ð¸Ñ… ÑÐ»Ð¾Ð²."
    )
    prompt = "Ð¡Ð¿Ð¸ÑÐ¾Ðº Ñ‚ÐµÐ¼ Ð½Ð° ÑÐµÐ³Ð¾Ð´Ð½Ñ:"
    if last_topics:
        prompt += (
            "\nÐ¢ÐµÐ¼Ñ‹ Ð½Ðµ Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€ÑÑ‚ÑŒÑÑ Ñ Ñ‚ÐµÐ¼Ð¸, Ñ‡Ñ‚Ð¾ Ð±Ñ‹Ð»Ð¸ Ð²Ñ‡ÐµÑ€Ð°. "
            f"Ð’Ñ‡ÐµÑ€Ð°ÑˆÐ½Ð¸Ðµ Ñ‚ÐµÐ¼Ñ‹: {', '.join(last_topics)}."
        )
    if knowledge_base:
        prompt += (
            "\nÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð¿Ñ€Ð¾ÑˆÐ»Ñ‹Ñ… Ð²Ñ‹Ð²Ð¾Ð´Ð¾Ð² (Ð½Ðµ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€ÑÐ¹ Ñ‚ÐµÐ¼Ñ‹ Ð¸ Ñ‚ÐµÐ·Ð¸ÑÑ‹):\n"
            f"{knowledge_base}"
        )
    print(f"DEBUG: Prompt sent to Editor: {prompt}")
    recent_lower = {_topic_key(t) for t in last_topics}
    rejected: list[str] = []
    for attempt in range(3):
        attempt_prompt = prompt
        if rejected:
            attempt_prompt += (
                "\nÐ¢Ñ‹ ÑƒÐ¶Ðµ Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°Ð»: "
                f"{', '.join(rejected)}. Ð”Ð°Ð¹ ÐÐžÐ’Ð«Ð• Ñ‚ÐµÐ¼Ñ‹."
            )
        raw_response = _openai_generate_with_system(system_prompt, attempt_prompt, TEXT_MODEL)
        print(f"DEBUG RAW TOPICS: {raw_response}")
        cleaned = raw_response
        cleaned = re.sub(r"(?i)ÐºÐ¾Ð½ÐµÑ‡Ð½Ð¾.*?:", "", cleaned)
        cleaned = cleaned.replace('"', " ").replace("'", " ")
        cleaned = cleaned.replace(".", " ").replace("â€¢", " ")
        cleaned = re.sub(r"\s+", " ", cleaned).strip()
        parts = [p.strip() for p in re.split(r"[,\n;]+", cleaned) if p.strip()]
        topics = _sanitize_topics(parts)
        if not topics:
            continue
        filtered = [t for t in topics if _topic_key(t) not in recent_lower]
        if len(filtered) >= count:
            return filtered[:count]
        if filtered:
            return filtered[:count]
        rejected.extend(topics)
    return []


def _append_recent_topic(topic: str) -> None:
    topic = topic.strip()
    if not topic:
        return
    file_path = os.path.join(os.getcwd(), "recent_topics.txt")
    try:
        if os.path.exists(file_path):
            with open(file_path, "r", encoding="utf-8") as f:
                existing = {line.strip() for line in f if line.strip()}
        else:
            existing = set()
        if topic in existing:
            return
        with open(file_path, "a", encoding="utf-8") as f:
            f.write(f"{topic}\n")
    except OSError:
        pass


def _build_image_prompt(image_scenario: str) -> str:
    theme = random.choice(IMAGE_THEMES)
    base = image_scenario.strip()
    if len(base) > 300:
        base = base[:300]
    scenario_hint = f"Topic hint: {base}. " if base else ""
    return (
        f"{theme} "
        f"{scenario_hint}"
        "Use cinematic lighting, 8k resolution, minimalist aesthetic. "
        "Strictly focus on ONE central object or person. "
        "Do not mix themes or add extra scientific props outside the chosen theme."
    )


def _generate_image_url(prompt: str) -> Optional[str]:
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("Missing OPENAI_API_KEY. Provide it to enable image generation.")
        return None
    payload = {
        "model": "dall-e-3",
        "prompt": prompt,
        "size": "1024x1024",
    }
    data = json.dumps(payload).encode("utf-8")
    req = request.Request(
        "https://api.openai.com/v1/images/generations", data=data, method="POST"
    )
    req.add_header("Content-Type", "application/json")
    req.add_header("Authorization", f"Bearer {api_key}")
    last_error = None
    for attempt in range(3):
        try:
            with request.urlopen(req, timeout=60) as response:
                body = response.read().decode("utf-8")
            parsed = json.loads(body)
            items = parsed.get("data", [])
            if not items:
                return None
            return items[0].get("url")
        except HTTPError as exc:
            last_error = exc
            retry_in = 2 + attempt * 2
            print(f"Image generation failed (HTTP {exc.code}). Retry in {retry_in}s.")
            time.sleep(retry_in)
    if last_error:
        raise last_error
    return None


def _openai_generate(prompt: str, model: str) -> str:
    payload = {
        "model": model,
        "temperature": 0.2,
        "messages": [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": prompt},
        ],
    }
    data = json.dumps(payload, ensure_ascii=False).encode("utf-8")
    req = request.Request(
        "https://api.openai.com/v1/chat/completions", data=data, method="POST"
    )
    req.add_header("Content-Type", "application/json")
    req.add_header("Authorization", f"Bearer {os.getenv('OPENAI_API_KEY')}")
    with request.urlopen(req, timeout=90) as response:
        body = response.read().decode("utf-8")
    parsed = json.loads(body)
    choices = parsed.get("choices", [])
    if not choices:
        return ""
    return (choices[0].get("message") or {}).get("content", "").strip()


def _openai_generate_with_system(system_prompt: str, prompt: str, model: str) -> str:
    payload = {
        "model": model,
        "temperature": 0.2,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt},
        ],
    }
    data = json.dumps(payload, ensure_ascii=False).encode("utf-8")
    req = request.Request(
        "https://api.openai.com/v1/chat/completions", data=data, method="POST"
    )
    req.add_header("Content-Type", "application/json")
    req.add_header("Authorization", f"Bearer {os.getenv('OPENAI_API_KEY')}")
    with request.urlopen(req, timeout=90) as response:
        body = response.read().decode("utf-8")
    parsed = json.loads(body)
    choices = parsed.get("choices", [])
    if not choices:
        return ""
    return (choices[0].get("message") or {}).get("content", "").strip()


def _extract_json(text: str) -> dict:
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        match = re.search(r"\{.*\}", text, re.DOTALL)
        if not match:
            raise
        return json.loads(match.group(0))


def _extract_source_metadata(source_text: str) -> dict:
    metadata = {}
    for line in source_text.splitlines():
        if line.startswith("SOURCE_JOURNAL:"):
            metadata["journal"] = line.split(":", 1)[1].strip()
        elif line.startswith("SOURCE_DOI:"):
            metadata["doi"] = line.split(":", 1)[1].strip()
        elif line.startswith("SOURCE_URL:"):
            metadata["url"] = line.split(":", 1)[1].strip()
        elif line.startswith("SOURCE_DATE:"):
            metadata["year"] = line.split(":", 1)[1].strip()
        elif line.startswith("SOURCE_AUTHORS:"):
            metadata["authors"] = line.split(":", 1)[1].strip()
        elif line.startswith("SOURCE_CITATIONS:"):
            raw = line.split(":", 1)[1].strip()
            metadata["citations_count"] = raw
    return metadata


def _inject_source_citation(parsed: dict, source_metadata: dict) -> dict:
    journal = source_metadata.get("journal", "").strip()
    doi = source_metadata.get("doi", "").strip()
    year = source_metadata.get("year", "").strip()
    authors = source_metadata.get("authors", "").strip()

    if not journal and not doi:
        return parsed

    citation_parts = []
    if authors:
        citation_parts.append(f"Authors: {authors}")
    if journal:
        citation_parts.append(f"Journal: {journal}")
    if year:
        citation_parts.append(f"Year: {year}")
    if doi:
        citation_parts.append(f"DOI: {doi}")
    citation = ". ".join(citation_parts).strip()

    existing_citation = str(parsed.get("study_citation", "")).strip()
    if not existing_citation:
        parsed["study_citation"] = citation
    elif citation.lower() not in existing_citation.lower():
        parsed["study_citation"] = f"{existing_citation} | {citation}"

    content_pro = str(parsed.get("content_pro", "")).strip()
    if content_pro:
        lower = content_pro.lower()
        if not any(token in lower for token in ("ÑÐ¿Ð¸ÑÐ¾Ðº Ð»Ð¸Ñ‚ÐµÑ€Ð°Ñ‚ÑƒÑ€Ñ‹", "references", "Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸")):
            content_pro = f"{content_pro}\n\nÐ¡Ð¿Ð¸ÑÐ¾Ðº Ð»Ð¸Ñ‚ÐµÑ€Ð°Ñ‚ÑƒÑ€Ñ‹:\n{citation}"
        elif citation.lower() not in lower:
            content_pro = f"{content_pro}\n{citation}"
        parsed["content_pro"] = content_pro

    if year and not str(parsed.get("study_year", "")).strip():
        parsed["study_year"] = year
    return parsed


def _extract_sample_size_from_text(text: str) -> str:
    return ""


def _postprocess_llm_output(parsed: dict, source_metadata: dict, source_text: str) -> dict:
    parsed = _inject_source_citation(parsed, source_metadata)
    content_pro = str(parsed.get("content_pro", "")).strip()
    study_year = str(parsed.get("study_year", "")).strip()
    sample_size = str(parsed.get("study_sample_size", "")).strip()

    if (source_metadata.get("doi") or source_metadata.get("url")) and not sample_size:
        sample_size = "Verified Scientific Report (DOI)"
        parsed["study_sample_size"] = sample_size

    if study_year and study_year not in content_pro:
        content_pro = f"Study Year: {study_year}\n{content_pro}"

    if (source_metadata.get("doi") or source_metadata.get("url")) and "Verification: Peer-reviewed study" not in content_pro:
        content_pro = f"Verification: Peer-reviewed study (DOI confirmed)\n{content_pro}"

    if sample_size and sample_size not in content_pro:
        label = "Sample size"
        if sample_size == "Verified Scientific Report (DOI)":
            label = "Ð¢Ð¸Ð¿ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ñ"
        content_pro = f"{label}: {sample_size}\n{content_pro}"

    parsed["content_pro"] = content_pro

    citation = str(parsed.get("study_citation", "")).strip()
    citation_lower = citation.lower()
    required_tokens = ("journal", "doi", "pubmed", "vol", "issue", "university")
    if not citation or not any(token in citation_lower for token in required_tokens):
        journal = source_metadata.get("journal", "").strip()
        doi = source_metadata.get("doi", "").strip()
        if journal or doi:
            citation_parts = []
            if journal:
                citation_parts.append(f"Journal: {journal}")
            if doi:
                citation_parts.append(f"DOI: {doi}")
            parsed["study_citation"] = ". ".join(citation_parts).strip()

    if parsed.get("_is_auto"):
        forced_citation = str(parsed.get("study_citation", "")).strip()
        lower = content_pro.lower()
        if not any(token in lower for token in ("ÑÐ¿Ð¸ÑÐ¾Ðº Ð»Ð¸Ñ‚ÐµÑ€Ð°Ñ‚ÑƒÑ€Ñ‹", "references", "Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸")):
            content_pro = f"{content_pro}\n\nÐ¡Ð¿Ð¸ÑÐ¾Ðº Ð»Ð¸Ñ‚ÐµÑ€Ð°Ñ‚ÑƒÑ€Ñ‹:\n{forced_citation}"
            parsed["content_pro"] = content_pro

    return parsed


def _is_generic_study_name(value: str) -> bool:
    lowered = value.lower()
    generic_phrases = (
        "study of peptide",
        "study of peptides",
        "peptide study",
        "peptides study",
        "clinical study",
        "clinical trial",
        "research on peptide",
        "research on peptides",
        "general study",
    )
    return not value.strip() or any(phrase in lowered for phrase in generic_phrases)


def _hard_filter(parsed: dict, filename: str) -> tuple[bool, str]:
    study_year = str(parsed.get("study_year", "")).strip()
    study_citation = str(parsed.get("study_citation", "")).strip()
    study_name = str(parsed.get("specific_study_name", "")).strip()
    content_pro = str(parsed.get("content_pro", "")).strip()
    sample_size = str(parsed.get("study_sample_size", "")).strip()
    is_auto = bool(parsed.get("_is_auto"))
    source_doi = str(parsed.get("_source_doi", "")).strip()

    # 1. ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð³Ð¾Ð´Ð° (Ð±Ð°Ð·Ð¾Ð²Ð°Ñ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚ÑŒ)
    if not re.fullmatch(r"(19|20)\d{2}", study_year):
        return False, "Rejected: Invalid year"

    # 2. ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ†Ð¸Ñ‚Ð°Ñ‚Ñ‹
    citation_lower = study_citation.lower()
    if not study_citation or "Ð½ÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ…" in citation_lower:
        return False, "Rejected: No citation"

    # Ð”Ð»Ñ Ð°Ð²Ñ‚Ð¾-Ñ„Ð°Ð¹Ð»Ð¾Ð² Ñ DOI Ð¼Ñ‹ Ð´Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÑƒ
    if not source_doi:
        required_tokens = ("journal", "doi", "pubmed", "vol", "issue", "university")
        if not any(token in citation_lower for token in required_tokens):
            if not (is_auto and re.search(r"\b(19|20)\d{2}\b", citation_lower)):
                return False, "Rejected: Citation missing key markers"

    # 3. ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Sample Size (Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÑƒ Ð¿Ñ€ÐµÐ´ÐºÐ»Ð¸Ð½Ð¸ÐºÐ¸)
    sample_lower = sample_size.lower()
    valid_sample_markers = [
        "verified", "scientific report", "model", "vitro", "vivo",
        "Ð¿Ñ€ÐµÐ´ÐºÐ»Ð¸Ð½Ð¸Ñ‡", "Ð»Ð¸Ð½Ð¸Ð¸", "Ð¶Ð¸Ð²Ð¾Ñ‚Ð½", "animal", "mice", "rats", "ÐºÑ€Ñ‹Ñ", "Ð¼Ñ‹ÑˆÐ¸"
    ]

    has_digits = bool(re.search(r"\d+", sample_size))
    is_valid_type = any(m in sample_lower for m in valid_sample_markers)

    if not (has_digits or is_valid_type):
        if is_auto:
            content_lower = content_pro.lower()
            clinical_markers = ("clinical", "trial", "study", "fda", "treatment")
            if any(marker in content_lower for marker in clinical_markers):
                sample_size = "clinical study (verified)"
                parsed["study_sample_size"] = sample_size
                if sample_size not in content_pro:
                    content_pro = f"Sample size: {sample_size}\n{content_pro}"
                    parsed["content_pro"] = content_pro
                print(f"DEBUG: Sample size after force: {sample_size}")
        if not re.search(r"\d+", sample_size) and not any(
            m in sample_size.lower() for m in valid_sample_markers
        ):
            return False, "Rejected: No sample size"

    # 4. ÐšÑ€Ð¾ÑÑ-Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ (ÐžÑÐ»Ð°Ð±Ð»ÑÐµÐ¼ Ð´Ð»Ñ Ð°Ð²Ñ‚Ð¾-Ñ„Ð°Ð¹Ð»Ð¾Ð²)
    content_lower = content_pro.lower()

    # Ð•ÑÐ»Ð¸ ÑÑ‚Ð¾ Ð°Ð²Ñ‚Ð¾-Ð¿Ð¾Ð¸ÑÐº, Ð½Ð°Ð¼ Ð´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð² Ñ‚ÐµÐºÑÑ‚Ðµ Ð±Ñ‹Ð» Ð“ÐžÐ” Ð¸ ÑƒÐ¿Ð¾Ð¼Ð¸Ð½Ð°Ð½Ð¸Ðµ ÐŸÐ•ÐŸÐ¢Ð˜Ð”Ð
    if is_auto:
        if study_year not in content_pro:
            return False, "Rejected: Year missing in text"
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ñ‡Ñ‚Ð¾ Ð² Ñ‚ÐµÐºÑÑ‚Ðµ ÐµÑÑ‚ÑŒ Ñ…Ð¾Ñ‚Ñ Ð±Ñ‹ Ñ‡Ð°ÑÑ‚ÑŒ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ñ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸Ð»Ð¸ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ ÑÐ»Ð¾Ð²Ð°
        study_keywords = [w for w in re.findall(r"[A-Za-zÐ-Ð¯Ð°-Ñ0-9-]+", study_name) if len(w) >= 5]
        if study_keywords and not any(word.lower() in content_lower for word in study_keywords):
            # Ð•ÑÐ»Ð¸ Ð½Ðµ Ð½Ð°ÑˆÐ»Ð¸ ÑÐ»Ð¾Ð²Ð° Ð¸Ð· Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°, Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ ÑÑÑ‹Ð»ÐºÐ¸ Ð¸Ð»Ð¸ DOI
            if not (source_doi.lower() in content_lower or "[" in content_pro):
                return False, "Rejected: Content does not match study metadata"
    else:
        # Ð”Ð»Ñ Ñ€ÑƒÑ‡Ð½Ñ‹Ñ… Ñ„Ð°Ð¹Ð»Ð¾Ð² Ð¾ÑÑ‚Ð°Ð²Ð»ÑÐµÐ¼ ÑÑ‚Ñ€Ð¾Ð³ÑƒÑŽ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÑƒ
        if "[" not in content_pro and "(" not in content_pro:
            return False, "Rejected: No references in body"
        if not any(token in content_pro for token in ("Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð»Ð¸Ñ‚ÐµÑ€Ð°Ñ‚ÑƒÑ€Ñ‹", "References", "Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸")):
            return False, "Rejected: No references in body"

    return True, ""


def _strip_html(text: str) -> str:
    cleaned = re.sub(r"(?is)<(script|style).*?>.*?</\1>", " ", text)
    cleaned = re.sub(r"(?is)<.*?>", " ", cleaned)
    cleaned = re.sub(r"\s+", " ", cleaned).strip()
    return cleaned


def _google_search_urls(query: str, max_results: int) -> list[str]:
    try:
        from googlesearch import search  # type: ignore
    except Exception:
        return []
    try:
        return list(search(query, num_results=max_results))
    except TypeError:
        return list(search(query, num=max_results))
    except Exception:
        return []


def _pubmed_search_urls(query: str, max_results: int, year_from: int = 2024, year_to: int = 2025) -> list[str]:
    term = parse.quote(f"{query} AND ({year_from}[dp]:{year_to}[dp])")
    endpoint = (
        "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
        f"?db=pubmed&retmode=json&retmax={max_results}&term={term}"
    )
    req = request.Request(endpoint, method="GET")
    with request.urlopen(req, timeout=30) as response:
        body = response.read().decode("utf-8")
    parsed = json.loads(body)
    ids = parsed.get("esearchresult", {}).get("idlist", [])
    return [f"https://pubmed.ncbi.nlm.nih.gov/{pid}/" for pid in ids if pid]


def _pubmed_fetch_records(ids: list[str]) -> list[dict]:
    if not ids:
        return []
    id_param = ",".join(ids)
    endpoint = (
        "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
        f"?db=pubmed&retmode=xml&id={id_param}"
    )
    req = request.Request(endpoint, method="GET")
    with request.urlopen(req, timeout=30) as response:
        body = response.read().decode("utf-8", errors="replace")
    root = ET.fromstring(body)
    records: list[dict] = []
    for article in root.findall(".//PubmedArticle"):
        journal_title = article.findtext(".//Journal/Title", default="").strip()
        year = article.findtext(".//PubDate/Year", default="").strip()
        doi = ""
        for eloc in article.findall(".//ELocationID"):
            if eloc.get("EIdType") == "doi":
                doi = (eloc.text or "").strip()
                break
        abstract_texts = [
            (elem.text or "").strip()
            for elem in article.findall(".//Abstract/AbstractText")
            if (elem.text or "").strip()
        ]
        abstract = " ".join(abstract_texts).strip()
        authors = []
        for author in article.findall(".//Author"):
            last = author.findtext("LastName", default="").strip()
            initials = author.findtext("Initials", default="").strip()
            if last and initials:
                authors.append(f"{last} {initials}")
            elif last:
                authors.append(last)
        records.append(
            {
                "journal": journal_title,
                "year": year,
                "doi": doi,
                "authors": ", ".join(authors),
                "abstract": abstract,
            }
        )
    return records


def _europe_pmc_search_records(query: str, max_results: int) -> list[dict]:
    q = parse.quote(query)
    endpoint = (
        "https://www.ebi.ac.uk/europepmc/webservices/rest/search"
        f"?query={q}&pageSize={max_results}&format=json"
    )
    try:
        req = request.Request(endpoint, method="GET")
        with request.urlopen(req, timeout=30) as response:
            body = response.read().decode("utf-8", errors="replace")
        parsed = json.loads(body)
    except Exception:
        return []
    results = parsed.get("resultList", {}).get("result", []) or []
    records: list[dict] = []
    for item in results:
        records.append(
            {
                "journal": (item.get("journalTitle") or "").strip(),
                "year": str(item.get("pubYear") or "").strip(),
                "doi": (item.get("doi") or "").strip(),
                "authors": (item.get("authorString") or "").strip(),
                "abstract": (item.get("abstractText") or "").strip(),
                "url": (item.get("fullTextUrlList", {}) or {}).get("fullTextUrl", [{}])[0].get("url", ""),
            }
        )
    return records


def _semantic_scholar_search_records(query: str, max_results: int) -> list[dict]:
    q = parse.quote(query)
    endpoint = (
        "https://api.semanticscholar.org/graph/v1/paper/search"
        f"?query={q}&limit={max_results}"
        "&fields=title,year,authors,venue,abstract,doi,url,citationCount"
    )
    try:
        req = request.Request(endpoint, method="GET")
        with request.urlopen(req, timeout=30) as response:
            body = response.read().decode("utf-8", errors="replace")
        parsed = json.loads(body)
    except Exception:
        return []
    results = parsed.get("data", []) or []
    records: list[dict] = []
    for item in results:
        authors = ", ".join(a.get("name", "") for a in item.get("authors", []) if a.get("name"))
        records.append(
            {
                "journal": (item.get("venue") or "").strip(),
                "year": str(item.get("year") or "").strip(),
                "doi": (item.get("doi") or "").strip(),
                "authors": authors.strip(),
                "abstract": (item.get("abstract") or "").strip(),
                "url": (item.get("url") or "").strip(),
                "citations_count": str(item.get("citationCount") or "").strip(),
            }
        )
    return records


def _clinicaltrials_search_records(query: str, max_results: int) -> list[dict]:
    q = parse.quote(query)
    endpoint = f"https://clinicaltrials.gov/api/v2/studies?query.term={q}&pageSize={max_results}"
    try:
        req = request.Request(endpoint, method="GET")
        with request.urlopen(req, timeout=30) as response:
            body = response.read().decode("utf-8", errors="replace")
        parsed = json.loads(body)
    except Exception:
        return []
    studies = parsed.get("studies", []) or []
    records: list[dict] = []
    for item in studies:
        ident = item.get("protocolSection", {}).get("identificationModule", {}) or {}
        descr = item.get("protocolSection", {}).get("descriptionModule", {}) or {}
        status = item.get("protocolSection", {}).get("statusModule", {}) or {}
        sponsor = item.get("protocolSection", {}).get("sponsorCollaboratorsModule", {}) or {}
        nct_id = ident.get("nctId", "")
        brief_title = ident.get("briefTitle", "")
        start_date = status.get("startDateStruct", {}) or {}
        record_url = f"https://clinicaltrials.gov/study/{nct_id}" if nct_id else ""
        records.append(
            {
                "journal": "ClinicalTrials.gov",
                "year": str(start_date.get("date", "")[:4]).strip(),
                "doi": record_url,
                "authors": (sponsor.get("leadSponsor", {}) or {}).get("name", ""),
                "abstract": (descr.get("briefSummary") or "").strip() or brief_title,
                "url": record_url,
            }
        )
    return records


def _fetch_page_text(url: str, timeout: int = 20) -> str:
    req = request.Request(url, method="GET")
    req.add_header("User-Agent", "Mozilla/5.0")
    with request.urlopen(req, timeout=timeout) as response:
        body = response.read().decode("utf-8", errors="replace")
    return _strip_html(body)


def _collect_search_snippets(keyword: str, max_results: int = 10) -> str:
    snippets: list[str] = []
    pubmed_urls = _pubmed_search_urls(keyword, 3)
    ids = [url.rstrip("/").split("/")[-1] for url in pubmed_urls if url]
    pubmed_records = _pubmed_fetch_records(ids)
    for record in pubmed_records:
        header_lines = [
            f"SOURCE_JOURNAL: {record.get('journal', '')}",
            f"SOURCE_DOI: {record.get('doi', '') or 'https://pubmed.ncbi.nlm.nih.gov/'}",
            f"SOURCE_DATE: {record.get('year', '')}",
            f"SOURCE_AUTHORS: {record.get('authors', '')}",
            "SOURCE_CITATIONS: ",
            "SOURCE_URL: https://pubmed.ncbi.nlm.nih.gov/",
        ]
        if not record.get("journal") or not record.get("year") or not record.get("doi"):
            header_lines.append("This is a formal academic record from PubMed database")
        abstract = record.get("abstract", "")
        if abstract:
            snippets.append("\n".join(header_lines) + "\n\n" + abstract)

    europe_records = _europe_pmc_search_records(keyword, 3)
    for record in europe_records:
        header_lines = [
            f"SOURCE_JOURNAL: {record.get('journal', '')}",
            f"SOURCE_DOI: {record.get('doi', '') or record.get('url', '')}",
            f"SOURCE_DATE: {record.get('year', '')}",
            f"SOURCE_AUTHORS: {record.get('authors', '')}",
            "SOURCE_CITATIONS: ",
            f"SOURCE_URL: {record.get('url', '')}",
        ]
        abstract = record.get("abstract", "")
        if abstract:
            snippets.append("\n".join(header_lines) + "\n\n" + abstract)

    semantic_records = _semantic_scholar_search_records(keyword, 3)
    for record in semantic_records:
        header_lines = [
            f"SOURCE_JOURNAL: {record.get('journal', '')}",
            f"SOURCE_DOI: {record.get('doi', '') or record.get('url', '')}",
            f"SOURCE_DATE: {record.get('year', '')}",
            f"SOURCE_AUTHORS: {record.get('authors', '')}",
            f"SOURCE_CITATIONS: {record.get('citations_count', '')}",
            f"SOURCE_URL: {record.get('url', '')}",
        ]
        abstract = record.get("abstract", "")
        if abstract:
            snippets.append("\n".join(header_lines) + "\n\n" + abstract)

    trial_records = _clinicaltrials_search_records(keyword, 3)
    for record in trial_records:
        header_lines = [
            f"SOURCE_JOURNAL: {record.get('journal', '')}",
            f"SOURCE_DOI: {record.get('doi', '') or record.get('url', '')}",
            f"SOURCE_DATE: {record.get('year', '')}",
            f"SOURCE_AUTHORS: {record.get('authors', '')}",
            "SOURCE_CITATIONS: ",
            f"SOURCE_URL: {record.get('url', '')}",
            "This is a formal academic record from ClinicalTrials.gov database",
        ]
        abstract = record.get("abstract", "")
        if abstract:
            snippets.append("\n".join(header_lines) + "\n\n" + abstract)

    if snippets:
        return "\n\n".join(snippets)

    queries = [
        f'site:pubmed.ncbi.nlm.nih.gov "{keyword}" 2024..2025',
        f'site:clinicaltrials.gov "{keyword}"',
        f'"{keyword}" peer-reviewed study 2024 journal doi',
    ]
    urls: list[str] = []
    for query in queries:
        urls.extend(_google_search_urls(query, max_results))
    unique_urls = list(dict.fromkeys(urls))[: max_results * len(queries)]
    required_tokens = ("journal", "abstract", "results", "doi:", "clinicaltrials")
    for url in unique_urls:
        try:
            text = _fetch_page_text(url)
        except Exception:
            continue
        if not any(token in text.lower() for token in required_tokens):
            continue
        hits = []
        for match in re.finditer(r"(?i)\b(2024|2025)\b", text):
            start = max(0, match.start() - 220)
            end = min(len(text), match.end() + 220)
            hits.append(text[start:end])
        if hits:
            snippet = " ... ".join(hits[:3])
            snippets.append(f"Source: {url}\n{snippet}")
        if len(snippets) >= max_results:
            break
    if not snippets:
        return ""
    header_lines = [
        "SOURCE_JOURNAL: ",
        "SOURCE_DOI: ",
        "SOURCE_DATE: ",
        "SOURCE_AUTHORS: ",
        "This is a formal academic record from PubMed database",
    ]
    return "\n".join(header_lines) + "\n\n" + "\n\n".join(snippets)


def _seed_research_db(db_path: str) -> None:
    for keyword in SEARCH_KEYWORDS:
        if not keyword or not re.search(r"[A-Za-zÐ-Ð¯Ð°-Ñ0-9]", keyword):
            continue
        filename = f"auto_{_normalize_name(keyword)}.txt"
        if "{" in filename or "}" in filename:
            continue
        file_path = os.path.join(db_path, filename)
        if os.path.exists(file_path):
            continue
        snippets = _collect_search_snippets(keyword)
        if not snippets:
            continue
        has_recent_year = re.search(r"\b(2024|2025)\b", snippets)
        has_numbers = re.search(r"\d{2,}", snippets)
        if not has_recent_year or not has_numbers:
            continue
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(snippets)


def _generate_article_versions(
    source_text: str, peptide_name: str, filename: str, include_knowledge_base: bool = True
) -> Optional[tuple[str, str, str, str]]:
    source_text = source_text.strip()
    short_source = len(source_text) < 200
    keyword_only = short_source and len(source_text.split()) <= 3
    source_metadata = _extract_source_metadata(source_text)
    knowledge_base = _load_knowledge_base() if include_knowledge_base else ""
    user_message = (
        "ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ Ñ‚ÐµÐºÑÑ‚ Ð¸ ÑÐ´ÐµÐ»Ð°Ð¹ Ð¸Ð· Ð½ÐµÐ³Ð¾ Pro Ð¸ Lite Ð²ÐµÑ€ÑÐ¸Ð¸:\n\n"
        f"{source_text}"
    )
    prompt = (
        "Ð’ÐµÑ€Ð½Ð¸ ÑÑ‚Ñ€Ð¾Ð³Ð¾ JSON Ñ Ð¿Ð¾Ð»ÑÐ¼Ð¸: title, content_pro, content_lite, "
        "image_scenario, should_publish, skip_reason, "
        "specific_study_name, study_year, study_citation, study_sample_size.\n"
        "title: Ð¼Ð°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð¾ ÐºÐ»Ð¸ÐºÐ°Ð±ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ð¸ Ñ…Ð°Ð¹Ð¿Ð¾Ð²Ñ‹Ð¹ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²Ð¾Ðº, "
        "Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ñ ÑÐ¼Ð¾Ð´Ð·Ð¸ ðŸ§¬, ðŸš€, ðŸ§ .\n"
        "content_pro: ÑÑ‚Ñ€Ð¾Ð³Ð¾ Ð°ÐºÐ°Ð´ÐµÐ¼Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð¸ ÑÑƒÑ…Ð¾Ð¹ ÑÑ‚Ð¸Ð»ÑŒ, Ð±ÐµÐ· Ð¾Ñ†ÐµÐ½Ð¾Ñ‡Ð½Ñ‹Ñ… ÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹. "
        "Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Markdown: Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¸ (##), **Ð¶Ð¸Ñ€Ð½Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚**, Ð¼Ð°Ñ€ÐºÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ ÑÐ¿Ð¸ÑÐºÐ¸ (*). "
        "ÐžÐ±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ ÑÐµÐºÑ†Ð¸Ð¸: "
        "## ÐžÐ±Ð·Ð¾Ñ€ ÑÑƒÐ±ÑÑ‚Ð°Ð½Ñ†Ð¸Ð¸, ## ÐœÐµÑ…Ð°Ð½Ð¸Ð·Ð¼ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ, ## Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ð¹, "
        "## Ð¡Ð¿Ñ€Ð°Ð²Ð¾Ñ‡Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ. "
        "Ð’Ð½ÑƒÑ‚Ñ€Ð¸ ÑÐµÐºÑ†Ð¸Ð¹: "
        "ÐŸÐ¾Ð»Ð½Ð¾Ðµ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ñ; Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð»Ð¾Ð³Ð¸Ñ (Ð´Ð¸Ð·Ð°Ð¹Ð½, Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐ° n, Ð´Ð¾Ð·Ð¸Ñ€Ð¾Ð²ÐºÐ¸, Ð´Ð»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ); "
        "Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ (ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÐµÐ»Ð¸, Ð´ÐµÐ»ÑŒÑ‚Ñ‹ Ð² %, p-Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ); "
        "Ð±Ð¸Ð¾Ñ…Ð¸Ð¼Ð¸Ñ (ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ðµ Ð¼Ð¾Ð»ÐµÐºÑƒÐ»ÑÑ€Ð½Ñ‹Ðµ Ð¼ÐµÑ…Ð°Ð½Ð¸Ð·Ð¼Ñ‹ Ð¸ Ð¿ÑƒÑ‚Ð¸); "
        "ÑÐ½Ð¾ÑÐºÐ° (Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº [1] Ð¸ DOI). "
        "Ð¦ÐµÐ»ÑŒ: Ð´Ð°Ñ‚ÑŒ Ñ‚Ð²ÐµÑ€Ð´Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð»Ñ Ð¿Ñ€Ð¸Ð½ÑÑ‚Ð¸Ñ Ñ€ÐµÑˆÐµÐ½Ð¸Ð¹. "
        "LaTeX Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ ÑÐ»Ð¾Ð¶Ð½Ñ‹Ñ… Ñ„Ð¾Ñ€Ð¼ÑƒÐ». "
        "ÐžÐ±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹ [1] Ð¸ ÑÐ¿Ð¸ÑÐ¾Ðº Ð»Ð¸Ñ‚ÐµÑ€Ð°Ñ‚ÑƒÑ€Ñ‹ Ð² ÐºÐ¾Ð½Ñ†Ðµ "
        "(Author, Journal, Year, DOI). "
        "ÐžÐ±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ ÑƒÐ¿Ð¾Ð¼ÑÐ½Ð¸ BioPeptidePlus.\n"
        "content_lite: ÑƒÐ¿Ñ€Ð¾Ñ‰ÐµÐ½Ð½Ñ‹Ð¹, ÐºÐ°Ñ‡ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ Ð½Ð°ÑƒÑ‡Ð¿Ð¾Ð¿ Ð±ÐµÐ· Ð¶ÐµÐ»Ñ‚Ð¸Ð·Ð½Ñ‹. "
        "Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Markdown Ð¸ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ðµ Ð°Ð±Ð·Ð°Ñ†Ñ‹. "
        "Ð’ Lite View Ð½Ð°Ñ‡Ð¸Ð½Ð°Ð¹ ÑÑ‚Ñ€Ð¾ÐºÐ¸ Ñ ÑÐ¼Ð¾Ð´Ð·Ð¸. "
        "ÐžÐ±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ ÑÐµÐºÑ†Ð¸Ð¸: "
        "## ÐžÐ±Ð·Ð¾Ñ€ ÑÑƒÐ±ÑÑ‚Ð°Ð½Ñ†Ð¸Ð¸, ## ÐœÐµÑ…Ð°Ð½Ð¸Ð·Ð¼ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ, ## Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ð¹, "
        "## Ð¡Ð¿Ñ€Ð°Ð²Ð¾Ñ‡Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ. "
        "Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð°: ÐšÑ€ÑŽÑ‡Ð¾Ðº, Ð¡ÑƒÑ‚ÑŒ, ÐŸÑ€Ð°ÐºÑ‚Ð¸ÐºÐ°, Ð¡Ñ‚Ð°Ñ‚ÑƒÑ, Ð¡Ð½Ð¾ÑÐºÐ° (Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº [1] Ð¸ DOI). "
        "Ð¦ÐµÐ»ÑŒ: Ð±Ñ‹ÑÑ‚Ñ€Ð¾ Ð¾Ð±ÑŠÑÑÐ½Ð¸Ñ‚ÑŒ Ð¾Ð±Ñ‹Ñ‡Ð½Ð¾Ð¼Ñƒ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÑƒ Ñ†ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ Ð²ÐµÑ‰ÐµÑÑ‚Ð²Ð°. "
        "ÐžÐ±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹ [1] Ð¸ ÑÐ¿Ð¸ÑÐ¾Ðº Ð»Ð¸Ñ‚ÐµÑ€Ð°Ñ‚ÑƒÑ€Ñ‹ Ð² ÐºÐ¾Ð½Ñ†Ðµ "
        "(Author, Journal, Year, DOI). "
        "ÐžÐ±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ ÑƒÐ¿Ð¾Ð¼ÑÐ½Ð¸ BioPeptidePlus.\n"
        "image_scenario: Ð¿Ñ€Ð¸Ð´ÑƒÐ¼Ð°Ð¹ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÐ¸ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ñ‚ÐµÐºÑÑ‚Ð°. "
        "ÐÐ• Ð´ÐµÐ»Ð°Ð¹ Ð²ÑÐµÐ³Ð´Ð° Ð°Ð±ÑÑ‚Ñ€Ð°ÐºÑ†Ð¸ÑŽ â€” Ñ‡ÐµÑ€ÐµÐ´ÑƒÐ¹ ÑÑ‚Ð¸Ð»Ð¸. "
        "Ð•ÑÐ»Ð¸ ÑƒÐ¿Ð¾Ð¼Ð¸Ð½Ð°ÐµÑ‚ÑÑ ÑƒÐ½Ð¸Ð²ÐµÑ€ÑÐ¸Ñ‚ÐµÑ‚/ÑÑ‚Ñ€Ð°Ð½Ð° â€” Ð´Ð¾Ð±Ð°Ð²ÑŒ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ñƒ "
        "(Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, 'British scientists near Big Ben style' Ð¸Ð»Ð¸ "
        "'Harvard campus background'). "
        "Ð•ÑÐ»Ð¸ Ñ€ÐµÑ‡ÑŒ Ð¾ Ð»ÑŽÐ´ÑÑ…/Ð¸ÑÐ¿Ñ‹Ñ‚Ð°Ð½Ð¸ÑÑ… â€” Ð¿Ð¾ÐºÐ°Ð¶Ð¸ ÑƒÑ‡ÐµÐ½Ñ‹Ñ… Ð² Ñ„ÑƒÑ‚ÑƒÑ€Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ "
        "Ð»Ð°Ð±Ð¾Ñ€Ð°Ñ‚Ð¾Ñ€Ð¸Ð¸, Ð´Ð¾ÐºÑ‚Ð¾Ñ€Ð¾Ð² Ð¸Ð»Ð¸ Ð±Ð¸Ð¾Ñ…Ð°ÐºÐµÑ€Ð¾Ð². "
        "Ð•ÑÐ»Ð¸ Ñ€ÐµÑ‡ÑŒ Ð¾ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ðµ Ð²ÐµÑ‰ÐµÑÑ‚Ð²Ð° â€” ÐºÑ€Ð°ÑÐ¸Ð²Ñ‹Ð¹ 3D Ð¼Ð°ÐºÑ€Ð¾-Ð¼Ð¸Ñ€. "
        "Ð¡Ñ‚Ð¸Ð»ÑŒ Ð²ÑÐµÐ³Ð´Ð°: Cinematic, Unreal Engine 5, Volumetric Lighting, "
        "Photorealistic but futuristic.\n"
        "should_publish: true/false. Ð•ÑÐ»Ð¸ Ð½ÐµÑ‚ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ð³Ð¾ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ñ, "
        "ÑÑ‚Ð°Ð²ÑŒ false Ð¸ Ð·Ð°Ð¿Ð¾Ð»Ð½ÑÐ¹ skip_reason.\n"
        "skip_reason: ÐºÑ€Ð°Ñ‚ÐºÐ¾ Ð¾Ð±ÑŠÑÑÐ½Ð¸, Ð¿Ð¾Ñ‡ÐµÐ¼Ñƒ Ð¿Ñ€Ð¾Ð¿ÑƒÑÐº.\n"
        "study_year: Ð³Ð¾Ð´ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸Ð· Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ°.\n"
        "study_citation: Ð»Ð¸Ñ‚ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð° Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ Author, Journal, Year, DOI.\n"
        "study_sample_size: Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐ° (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€: 120 Ð¿Ð°Ñ†Ð¸ÐµÐ½Ñ‚Ð¾Ð², 40 Ð¼Ñ‹ÑˆÐµÐ¹, in vitro).\n"
        "specific_study_name: Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ð³Ð¾ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸Ð· Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ°.\n"
        "Ð•ÑÐ»Ð¸ ÑÐ¿Ð¸ÑÐ¾Ðº biological targets Ð¿ÑƒÑÑ‚, Ð¿Ð¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»Ð¸Ñ‚ÑŒ 1-2 Ð¾ÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹ "
        "Ð¾Ñ€Ð³Ð°Ð½Ð¸Ð·Ð¼Ð° Ð¸Ð· ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð° (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, Ð¼Ð¾Ð·Ð³, ÑÐµÑ€Ð´Ñ†Ðµ, Ð¼ÐµÑ‚Ð°Ð±Ð¾Ð»Ð¸Ð·Ð¼) Ð¸ Ð¾Ñ‚Ñ€Ð°Ð·Ð¸ ÑÑ‚Ð¾ "
        "Ð² Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ñ….\n"
        "Ð˜Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ñ Ð´Ð»Ñ Dr. Drug: ÑÑ€Ð°Ð²Ð½Ð¸Ð²Ð°Ð¹ Ð½Ð¾Ð²ÑƒÑŽ ÑÑ‚Ð°Ñ‚ÑŒÑŽ Ñ Ð¿Ñ€Ð¾ÑˆÐ»Ñ‹Ð¼Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸ "
        "Ð¸Ð· knowledge_base.txt. Ð•ÑÐ»Ð¸ ÐµÑÑ‚ÑŒ Ð¿Ñ€Ð¾Ñ‚Ð¸Ð²Ð¾Ñ€ÐµÑ‡Ð¸Ñ Ð¸Ð»Ð¸ ÑÐ¸Ð½ÐµÑ€Ð³Ð¸Ñ "
        "(Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, BPC-157 ÑƒÑÐ¸Ð»Ð¸Ð²Ð°ÐµÑ‚ ÑÑ„Ñ„ÐµÐºÑ‚ Ð½Ð¾Ð²Ð¾Ð³Ð¾ Ð²ÐµÑ‰ÐµÑÑ‚Ð²Ð°) â€” Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ "
        "ÑƒÐºÐ°Ð¶Ð¸ ÑÑ‚Ð¾ Ð² Ð±Ð»Ð¾ÐºÐµ PRO.\n"
        "Ð˜Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ñ Ð´Ð»Ñ ÐÑ€Ð±Ð¸Ñ‚Ñ€Ð°: ÐºÑ€Ð¸Ñ‚ÐµÑ€Ð¸Ð¸ ÑÑ‚Ñ€Ð¾Ð³Ð¾ÑÑ‚Ð¸ Ñ€Ð°ÑÑ‚ÑƒÑ‚. "
        "Ð•ÑÐ»Ð¸ Ð¼Ñ‹ ÑƒÐ¶Ðµ Ð¿Ð¸ÑÐ°Ð»Ð¸ Ð¾ Ð¿Ð¾Ð´Ð¾Ð±Ð½Ð¾Ð¼ Ð²ÐµÑ‰ÐµÑÑ‚Ð²Ðµ Ñ Ð»ÑƒÑ‡ÑˆÐµÐ¹ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐ¾Ð¹, "
        "Ñ‚Ñ€ÐµÐ±ÑƒÐ¹ Ð¾Ñ‚ Ð½Ð¾Ð²Ð¾Ð³Ð¾ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ñ Ð±Ð¾Ð»ÐµÐµ Ð²ÐµÑÐºÐ¸Ñ… Ð´Ð¾ÐºÐ°Ð·Ð°Ñ‚ÐµÐ»ÑŒÑÑ‚Ð². "
        "Ð’ ÑÑ‚Ð¾Ð¼ ÑÐ»ÑƒÑ‡Ð°Ðµ Ð´Ð¾Ð±Ð°Ð²ÑŒ Ð² PRO ÑÑ‚Ñ€Ð¾ÐºÑƒ 'Ð¡Ñ‚Ð°Ñ‚ÑƒÑ: Ð¢Ñ€ÐµÐ±ÑƒÐµÑ‚ Ð¿Ð¾Ð´Ñ‚Ð²ÐµÑ€Ð¶Ð´ÐµÐ½Ð¸Ñ'.\n"
        f"Ð¢ÐµÐ¼Ð°: {peptide_name}\n\n"
        f"{user_message}"
    )
    if knowledge_base:
        prompt += (
            "\n\nÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ knowledge_base.txt (Ð´Ð»Ñ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸Ñ Ð¸ ÑÐ¸Ð½ÐµÑ€Ð³Ð¸Ð¹):\n"
            f"{knowledge_base}"
        )
    if short_source:
        prompt += (
            "\n\nÐ˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¹ Ð¸Ð»Ð¸ Ð¿ÑƒÑÑ‚Ð¾Ð¹. "
            "ÐœÐ¾Ð¶Ð½Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð²Ð½ÐµÑˆÐ½Ð¸Ðµ Ð·Ð½Ð°Ð½Ð¸Ñ, Ð½Ð¾ Ñ‡ÐµÑÑ‚Ð½Ð¾ Ð½Ð°Ñ‡Ð½Ð¸ Ñ Ñ„Ñ€Ð°Ð·Ñ‹: "
            "'ÐŸÐ¾ Ð´Ð°Ð½Ð½Ñ‹Ð¼ Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð²...'. "
            "ÐÐµ ÑƒÐºÐ°Ð·Ñ‹Ð²Ð°Ð¹ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ðµ Ñ†Ð¸Ñ„Ñ€Ñ‹, Ð³Ð¾Ð´Ñ‹, Ð²Ñ‹Ð±Ð¾Ñ€ÐºÐ¸ Ð¸ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹, "
            "ÐµÑÐ»Ð¸ Ð¾Ð½Ð¸ Ð½Ðµ Ð¿Ð¾Ð´Ñ‚Ð²ÐµÑ€Ð¶Ð´ÐµÐ½Ñ‹ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ°Ð¼Ð¸ Ð² Ñ‚ÐµÐºÑÑ‚Ðµ."
        )
    if keyword_only:
        prompt += (
            "\n\nÐ’Ñ…Ð¾Ð´ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐºÐ»ÑŽÑ‡ÐµÐ²Ð¾Ðµ ÑÐ»Ð¾Ð²Ð¾. "
            "Ð¡Ð´ÐµÐ»Ð°Ð¹ Ñ‡ÐµÑÑ‚Ð½ÑƒÑŽ ÑÐ¿Ñ€Ð°Ð²ÐºÑƒ Ð±ÐµÐ· Ð½ÐµÐ¿Ð¾Ð´Ñ‚Ð²ÐµÑ€Ð¶Ð´ÐµÐ½Ð½Ð¾Ð¹ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð¸ÐºÐ¸. "
            "Ð•ÑÐ»Ð¸ Ð½ÐµÑ‚ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼Ñ‹Ñ… ÑÑÑ‹Ð»Ð¾Ðº, ÑÐ²Ð½Ð¾ ÑƒÐºÐ°Ð¶Ð¸: "
            "'Ð¡ÑÑ‹Ð»ÐºÐ¸: Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð² Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐµ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‚'."
        )
    raw = _openai_generate(prompt, TEXT_MODEL)
    debug_files = {"auto_bpc-157.txt", "auto_epitalon.txt"}
    if filename in debug_files:
        print(f"=== RAW OUTPUT [{filename}] ===")
        print(raw)
    parsed = _extract_json(raw)
    if filename in debug_files:
        print(f"=== PARSED JSON [{filename}] ===")
        print(json.dumps(parsed, ensure_ascii=False, indent=2))
        print(f"=== content_pro [{filename}] ===")
        print(str(parsed.get("content_pro", "")))
    if not parsed:
        print(f"Skipping {peptide_name}: empty JSON response.")
        return None
    if source_metadata:
        parsed = _postprocess_llm_output(parsed, source_metadata, source_text)
        parsed["_source_doi"] = source_metadata.get("doi", "")
    if filename.startswith("auto_"):
        parsed["_is_auto"] = True
    should_publish = bool(parsed.get("should_publish", True))
    if not should_publish:
        reason = str(parsed.get("skip_reason", "")).strip() or "No study found."
        print(f"Skipping {peptide_name}: {reason}")
        return None
    ok, reject_reason = _hard_filter(parsed, peptide_name)
    if not ok:
        print(f"\033[31mSKIPPED [{peptide_name}]: {reject_reason}\033[0m")
        return None
    title = str(parsed.get("title", "")).strip()
    content_pro = str(parsed.get("content_pro", "")).strip()
    content_lite = str(parsed.get("content_lite", "")).strip()
    image_scenario = str(parsed.get("image_scenario", "")).strip()
    if not title or not content_pro or not content_lite:
        raise ValueError("GPT response missing required fields")
    if not image_scenario:
        image_scenario = title
    return title, content_pro, content_lite, image_scenario


def _send_journal_post(payload: dict) -> dict:
    data = json.dumps(payload).encode("utf-8")
    req = request.Request(
        JOURNAL_ENDPOINT,
        data=data,
        method="POST",
        headers={"Content-Type": "application/json"},
    )
    with request.urlopen(req, timeout=30) as response:
        return json.loads(response.read().decode("utf-8"))


def _prepare_lovable_payload(payload: dict) -> dict:
    allowed_fields = {
        "title",
        "content",
        "content_lite",
        "category",
        "tags",
        "image_url",
        "doi",
        "evidence_level",
    }
    tags = list(payload.get("tags") or [])
    if payload.get("biological_targets"):
        for item in payload.get("biological_targets") or []:
            if item not in tags:
                tags.append(item)
    payload["tags"] = tags
    return {k: v for k, v in payload.items() if k in allowed_fields}


def _send_telegram_update(image_url: Optional[str], text: str) -> None:
    token = (
        os.getenv("TELEGRAM_BOT_TOKEN")
        or os.getenv("DR_DRAG_TOKEN")
        or os.getenv("ARBITER_TOKEN")
    )
    chat_id = os.getenv("TELEGRAM_CHANNEL_ID")
    if not token or not chat_id:
        print("Telegram Ð¾Ñ‚Ð¿Ñ€Ð°Ð²ÐºÐ° Ð¿Ñ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½Ð°: Ð½ÐµÑ‚ TELEGRAM_BOT_TOKEN/CHANNEL_ID.")
        return
    if image_url:
        send_photo(token, chat_id, image_url, "", article_url=None)
    send_message(token, chat_id, text, article_url=None)


def main() -> int:
    parser = argparse.ArgumentParser(description="Auto research generator")
    parser.add_argument("peptide_name", nargs="*")
    parser.add_argument("--regen-db", action="store_true")
    parser.add_argument("--resume-after", default="")
    parser.add_argument("--topic", default="", help="Direct search query for research")
    args = parser.parse_args()
    if args.topic:
        args.regen_db = True

    db_path = os.path.join(os.getcwd(), "research_db")
    os.makedirs(db_path, exist_ok=True)
    if args.topic:
        topic_query = args.topic.strip()
        if not topic_query:
            print("Empty topic query.")
            return 1
        filename = f"auto_{_normalize_name(topic_query)}.txt"
        file_path = os.path.join(db_path, filename)
        snippets = _collect_search_snippets(topic_query)
        if not snippets:
            print("No research snippets found for topic.")
            return 1
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(snippets)
        peptide_name = _pretty_name(topic_query)
        try:
            generated = _generate_article_versions(
                snippets, peptide_name, filename, include_knowledge_base=False
            )
            if not generated:
                print("No publishable study found; skipping.")
                return 0
            title, content_pro, content_lite, image_scenario = generated
            source_metadata = _extract_source_metadata(snippets)
            image_prompt = _build_image_prompt(image_scenario)
            image_url = _generate_image_url(image_prompt)
            print("Image URL:", image_url)
            combined_text = f"{title}\n{content_pro}\n{content_lite}\n{snippets}"
            evidence_level = _detect_evidence_level(combined_text)
            results_block = _extract_results_block(content_pro)
            biological_targets = _extract_biological_targets(results_block)
            if not biological_targets:
                biological_targets = _infer_system_targets(f"{content_pro}\n{content_lite}")
            tags = _generate_tags(peptide_name, biological_targets)
            doi = _extract_doi(combined_text, source_metadata)
            citations_count = _parse_citations_count(source_metadata)
            payload = {
                "title": title,
                "content": content_pro,
                "content_lite": content_lite,
                "category": "science",
                "is_published": True,
                "image_url": image_url,
                "evidence_level": evidence_level,
                "biological_targets": biological_targets,
                "tags": tags,
                "doi": doi,
                "citations_count": citations_count,
            }
            response = _send_journal_post(payload)
            _send_telegram_update(image_url, content_lite)
            key_finding = _extract_key_finding(content_pro, content_lite)
            citation_hint = _extract_citation_hint(content_pro)
            _append_knowledge_base(peptide_name, key_finding, citation_hint)
            _append_recent_topic(peptide_name)
            print(f"{filename}: {response}")
            return 0
        except HTTPError as exc:
            body = exc.read().decode("utf-8", errors="replace").strip()
            print(body or f"HTTP {exc.code}")
            return 1
        except Exception as exc:
            print(f"Error processing {filename}: {exc}")
            return 1

    if args.regen_db:
        global SEARCH_KEYWORDS
        if args.topic:
            SEARCH_KEYWORDS = [args.topic.strip()]
            print(f"Using direct topic: {SEARCH_KEYWORDS[0]}")
            last_topics = _load_last_topics()
            recent_topic_keys = {_topic_key(t) for t in last_topics}
        else:
            last_topics = _load_last_topics()
            generated_topics = generate_daily_topics(last_topics, count=5)
            if generated_topics:
                SEARCH_KEYWORDS = generated_topics
                print(f"Generated topics: {', '.join(SEARCH_KEYWORDS)}")
            else:
                SEARCH_KEYWORDS = DEFAULT_KEYWORDS[:5]
                print(f"Generated topics empty; fallback to: {', '.join(SEARCH_KEYWORDS)}")
            recent_topic_keys = {_topic_key(t) for t in last_topics}

    if args.regen_db:
        entries = [
            f
            for f in os.listdir(db_path)
            if f.lower().endswith(".txt") and os.path.isfile(os.path.join(db_path, f))
        ]
        if not entries:
            _seed_research_db(db_path)
            entries = [
                f
                for f in os.listdir(db_path)
                if f.lower().endswith(".txt") and os.path.isfile(os.path.join(db_path, f))
            ]
        if not entries:
            print("No research_db entries found.")
            return 1
        resume_after = args.resume_after.strip().lower()
        processed = 0
        for filename in sorted(entries):
            if resume_after and filename.lower() <= resume_after:
                continue
            if DAILY_LIMIT and processed >= DAILY_LIMIT:
                print(f"Daily limit reached: {DAILY_LIMIT}")
                break
            if "{" in filename or "}" in filename:
                print(f"Skipping invalid filename: {filename}")
                continue
            file_path = os.path.join(db_path, filename)
            raw_name = os.path.splitext(filename)[0]
            topic_name = raw_name
            if topic_name.startswith("auto_"):
                topic_name = topic_name[len("auto_"):]
            peptide_name = _pretty_name(topic_name)
            if _topic_key(peptide_name) in recent_topic_keys:
                print(f"Skipping recent topic: {peptide_name}")
                continue
            with open(file_path, "r", encoding="utf-8") as f:
                source_text = f.read().strip()
            if not source_text:
                print(f"Skipping empty file: {filename}")
                continue
            try:
                generated = _generate_article_versions(source_text, peptide_name, filename)
                if not generated:
                    if filename.startswith("auto_"):
                        try:
                            os.remove(file_path)
                        except OSError:
                            pass
                    continue
                title, content_pro, content_lite, image_scenario = generated
                source_metadata = _extract_source_metadata(source_text)
                image_prompt = _build_image_prompt(image_scenario)
                image_url = _generate_image_url(image_prompt)
                print("Image URL:", image_url)
                combined_text = f"{title}\n{content_pro}\n{content_lite}\n{source_text}"
                evidence_level = _detect_evidence_level(combined_text)
                results_block = _extract_results_block(content_pro)
                biological_targets = _extract_biological_targets(results_block)
                if not biological_targets:
                    biological_targets = _infer_system_targets(f"{content_pro}\n{content_lite}")
                tags = _generate_tags(peptide_name, biological_targets)
                doi = _extract_doi(combined_text, source_metadata)
                citations_count = _parse_citations_count(source_metadata)
                payload = {
                    "title": title,
                    "content": content_pro,
                    "content_lite": content_lite,
                    "category": "science",
                    "is_published": True,
                    "image_url": image_url,
                    "evidence_level": evidence_level,
                    "biological_targets": biological_targets,
                    "tags": tags,
                    "doi": doi,
                    "citations_count": citations_count,
                }
                payload = _prepare_lovable_payload(payload)
                try:
                    response = _send_journal_post(payload)
                except Exception as exc:
                    print(f"Lovable API error for {filename}: {exc}")
                    continue
                if image_url:
                    print(f"Lovable image_url sent: {image_url}")
                _send_telegram_update(image_url, content_lite)
                key_finding = _extract_key_finding(content_pro, content_lite)
                citation_hint = _extract_citation_hint(content_pro)
                _append_knowledge_base(peptide_name, key_finding, citation_hint)
                _append_recent_topic(peptide_name)
                post_id = (response.get("post", {}) or {}).get("id") if isinstance(response, dict) else None
                if post_id:
                    try:
                        with open(file_path, "a", encoding="utf-8") as f:
                            f.write(f"\nPOST_ID: {post_id}\n")
                    except OSError:
                        pass
            except HTTPError as exc:
                body = exc.read().decode("utf-8", errors="replace").strip()
                print(body or f"HTTP {exc.code}")
                continue
            except Exception as exc:
                print(f"Error processing {filename}: {exc}")
                continue
            processed += 1
            print(f"{filename}: {response}")
        return 0

    peptide_name = " ".join(args.peptide_name).strip()
    if not peptide_name:
        print("Empty peptide name.")
        return 1

    key = _normalize_name(peptide_name)
    data = _mock_search(key)
    entry = _format_entry(peptide_name, data)

    file_path = os.path.join(db_path, f"{key}.txt")
    mode = "a" if os.path.exists(file_path) else "w"
    with open(file_path, mode, encoding="utf-8") as f:
        if mode == "a":
            f.write("\n")
        f.write(entry)

    print(f"âœ… Ð¤Ð°Ð¹Ð» Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½: {file_path}")

    generated = _generate_article_versions(entry, peptide_name, key)
    if not generated:
        print("No publishable study found; skipping.")
        return 0
    title, content_pro, content_lite, image_scenario = generated
    image_prompt = _build_image_prompt(image_scenario)
    image_url = _generate_image_url(image_prompt)
    print("Image URL:", image_url)
    combined_text = f"{title}\n{content_pro}\n{content_lite}\n{entry}"
    evidence_level = _detect_evidence_level(combined_text)
    results_block = _extract_results_block(content_pro)
    biological_targets = _extract_biological_targets(results_block)
    if not biological_targets:
        biological_targets = _infer_system_targets(f"{content_pro}\n{content_lite}")
    tags = _generate_tags(peptide_name, biological_targets)
    doi = _extract_doi(combined_text, {})
    citations_count = None
    payload = {
        "title": title,
        "content": content_pro,
        "content_lite": content_lite,
        "category": "science",
        "is_published": True,
        "image_url": image_url,
        "evidence_level": evidence_level,
        "biological_targets": biological_targets,
        "tags": tags,
        "doi": doi,
        "citations_count": citations_count,
    }
    try:
        payload = _prepare_lovable_payload(payload)
        response = _send_journal_post(payload)
    except HTTPError as exc:
        body = exc.read().decode("utf-8", errors="replace").strip()
        print(body or f"HTTP {exc.code}")
        return 1
    except Exception as exc:
        print(f"Lovable API error: {exc}")
        return 1
    if image_url:
        print(f"Lovable image_url sent: {image_url}")
    _send_telegram_update(image_url, content_lite)
    key_finding = _extract_key_finding(content_pro, content_lite)
    citation_hint = _extract_citation_hint(content_pro)
    _append_knowledge_base(peptide_name, key_finding, citation_hint)
    _append_recent_topic(peptide_name)
    post_id = (response.get("post", {}) or {}).get("id") if isinstance(response, dict) else None
    if post_id:
        try:
            with open(file_path, "a", encoding="utf-8") as f:
                f.write(f"\nPOST_ID: {post_id}\n")
        except OSError:
            pass
    print("Server response:", response)
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
