from __future__ import annotations

import asyncio
import json
import os
import re
import sys

from dotenv import load_dotenv
from telegram import Update
from telegram.ext import ApplicationBuilder, ContextTypes, MessageHandler, filters

load_dotenv()

# –ê–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –∫ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
DB_PATH = os.path.join(PROJECT_ROOT, "research_db")

URL_REGEX = re.compile(r"(https?://|www\.)\S+", re.IGNORECASE)
MENTION_REGEX = re.compile(r"@\w+", re.IGNORECASE)

BANNED_PHRASES = [
    "–∫—É–ø–∏—Ç—å –∑–¥–µ—Å—å",
    "–≤ –ª–∏—á–∫—É",
    "–ø–µ—Ä–µ—Ö–æ–¥–∏—Ç–µ",
    "–ª—É—á—à–∞—è —Ü–µ–Ω–∞",
    "–∏–¥–∏–æ—Ç",
    "–¥—É—Ä–∞–∫",
    "—Ç—É–ø–æ–π",
    "–º—Ä–∞–∑—å",
    "—É–±–ª—é–¥–æ–∫",
    "–Ω–µ–Ω–∞–≤–∏–∂—É",
]

WARNING_TEXT = (
    "‚ùå –ù–∞—Ä—É—à–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª BioPeptidePlus. –†–µ–∫–ª–∞–º–∞, —Å–ø–∞–º –∏ –ø–æ–ø—ã—Ç–∫–∏ –ø–µ—Ä–µ–º–∞–Ω–∏–≤–∞–Ω–∏—è "
    "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∑–∞–ø—Ä–µ—â–µ–Ω—ã. –ü–æ–≤—Ç–æ—Ä–Ω–æ–µ –Ω–∞—Ä—É—à–µ–Ω–∏–µ ‚Äî –±–∞–Ω"
)


def _scan_db_files(db_dir: str) -> dict[str, str]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å: –±–∞–∑–æ–≤—ã–π_–Ω–µ–π–º_—Ñ–∞–π–ª–∞ (lowercase) -> –ø–æ–ª–Ω–æ–µ_–∏–º—è_—Ñ–∞–π–ª–∞
    """
    if not os.path.isdir(db_dir):
        return {}
    files = [
        f
        for f in os.listdir(db_dir)
        if f.lower().endswith(".txt") and os.path.isfile(os.path.join(db_dir, f))
    ]
    # –ö–ª—é—á ‚Äî –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ (–±–µ–∑ .txt), –∑–Ω–∞—á–µ–Ω–∏–µ ‚Äî –∏–º—è —Ñ–∞–π–ª–∞
    return {_normalize_token(os.path.splitext(f)[0]): f for f in files}


def _normalize_token(text: str) -> str:
    lowered = text.lower()
    return re.sub(r"[-\s]+", "", lowered)


def _normalize_keyword(word: str) -> str:
    lowered = word.lower()
    ru_map = {
        "—Ç–∏—Ä–∑–µ–ø–∞—Ç–∏–¥": "tirzepatide",
        "—Å–µ–º–∞–∫—Å": "semax",
        "—Å–µ–ª–∞–Ω–∫": "selank",
        "–±–ø–∫157": "bpc157",
        "—Ç–±500": "tb500",
    }
    mapped = ru_map.get(lowered, lowered)
    return _normalize_token(mapped)


def _is_violation(text: str) -> bool:
    lowered = text.lower()
    if URL_REGEX.search(text):
        return True
    if MENTION_REGEX.search(text):
        return True
    return any(phrase in lowered for phrase in BANNED_PHRASES)


async def _delete_warning_later(
    context: ContextTypes.DEFAULT_TYPE, chat_id: int, msg_id: int
) -> None:
    await asyncio.sleep(10)
    try:
        await context.bot.delete_message(chat_id=chat_id, message_id=msg_id)
    except Exception:
        pass


async def _delete_notice_later(
    context: ContextTypes.DEFAULT_TYPE, chat_id: int, msg_id: int
) -> None:
    await asyncio.sleep(5)
    try:
        await context.bot.delete_message(chat_id=chat_id, message_id=msg_id)
    except Exception:
        pass


async def _is_admin(context: ContextTypes.DEFAULT_TYPE, chat_id: int, user_id: int) -> bool:
    try:
        admins = await context.bot.get_chat_administrators(chat_id)
    except Exception:
        return False
    return any(admin.user.id == user_id for admin in admins)

def get_peptide_info(query: str, db_dir: str = DB_PATH) -> str:
    """
    –ò—â–µ—Ç —Ñ–∞–π–ª {–Ω–∞–∑–≤–∞–Ω–∏–µ}.txt –ø–æ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –¥–µ—Ñ–∏—Å—ã –∏ –ø—Ä–æ–±–µ–ª—ã, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä—É—Å—Å–∫–∏–π –≤–≤–æ–¥.
    """
    print(f"[–õ–û–ì] –ü–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ: {query!r}")

    if not os.path.isdir(db_dir):
        print(f"[–õ–û–ì] research_db –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {db_dir}")
        return "‚öñÔ∏è –í –º–æ–µ–π –±–∞–∑–µ –ø–æ–∫–∞ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ —ç—Ç–∏–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º, –Ω–æ —è –º–æ–≥—É –ø–æ–∏—Å–∫–∞—Ç—å –∏—Ö –≤ —Å–µ—Ç–∏. –ù–∞–π—Ç–∏?"

    files_map = _scan_db_files(db_dir)
    current_files = sorted(files_map.values())

    raw_lower = query.lower().strip()
    stop_words = {"–ø—Ä–æ—Ç–æ–∫–æ–ª", "–∏–Ω—Ñ–æ", "—Å–ø—Ä–∞–≤–∫–∞"}
    for word in stop_words:
        raw_lower = raw_lower.replace(word, " ")
    raw_lower = " ".join(raw_lower.split())
    MAPPING = {
        "—Ç–∏—Ä–∑–µ–ø–∞—Ç–∏–¥": "tirzepatide",
        "—Å–µ–º–∞–∫—Å": "semax",
        "—Å–µ–ª–∞–Ω–∫": "selank",
        "—ç–ø–∏—Ç–∞–ª–æ–Ω": "epitalon",
        "–±–ø–∫157": "bpc157",
        "—Ç–±500": "tb500",
    }

    if raw_lower in MAPPING:
        normalized = MAPPING[raw_lower]
    else:
        normalized = " ".join(query.lower().split())
    normalized = _normalize_token(normalized)
    if "motsc" in normalized:
        normalized = "mots-c"

    words = re.findall(r"\w+", normalized)
    name = _normalize_keyword(words[0]) if words else _normalize_keyword(normalized)
    normalized_name = _normalize_token(name)

    resolved = files_map.get(normalized_name)
    file_name = resolved or f"{normalized_name}.txt"
    file_path = os.path.join(db_dir, file_name)

    print(f"DEBUG: –ò—â—É —Ñ–∞–π–ª {file_name} –ø–æ –ø—É—Ç–∏ {file_path}")
    print(f"–ê–†–ë–ò–¢–† –ò–©–ï–¢ –¢–£–¢: {file_path}")

    if not os.path.isfile(file_path):
        print(f"DEBUG: –ù–µ –Ω–∞—à–µ–ª {file_name} –≤ {db_dir}")
        print(f"DEBUG: –í –ø–∞–ø–∫–µ —Å–µ–π—á–∞—Å –ª–µ–∂–∞—Ç —Ñ–∞–π–ª—ã: {current_files}")
        return "–í –±–∞–∑–µ BioPeptidePlus –ø–æ–∫–∞ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ —ç—Ç–æ–º—É –∑–∞–ø—Ä–æ—Å—É"

    print("DEBUG: –§–∞–π–ª –Ω–∞–π–¥–µ–Ω, –æ—Ç–ø—Ä–∞–≤–ª—è—é –¥–∞–Ω–Ω—ã–µ")
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            file_contents = f.read()

        cleaned = _clean_text(file_contents)
        peptide_name = os.path.splitext(file_name)[0]
        if not cleaned.strip():
            return "–§–∞–π–ª –Ω–∞–π–¥–µ–Ω, –Ω–æ –¥–∞–Ω–Ω—ã—Ö –≤–Ω—É—Ç—Ä–∏ –ø–æ–∫–∞ –Ω–µ—Ç"

        return f"üî¨ **–≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ BioPeptidePlus: {peptide_name}**\n\n{cleaned.strip()}"
    except Exception as e:
        print(f"[–û–®–ò–ë–ö–ê] –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
        return f"‚öñÔ∏è –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π. ({e})"


def _clean_text(text: str) -> str:
    lines = []
    for raw in text.splitlines():
        line = raw.strip()
        if not line:
            continue
        if line.startswith("---"):
            continue
        if line.startswith("AUTO ENTRY"):
            continue
        if "AUTO ENTRY" in line or "DATA ENTRY" in line:
            continue
        if re.search(r"\b\d{4}[-/.]\d{2}[-/.]\d{2}\b", line):
            continue
        if re.search(r"\b\d{8,}\b", line):
            continue
        if line.startswith("‚öñÔ∏è"):
            continue
        lines.append(line)
    return "\n".join(lines)




async def _handle_update(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ Telegram –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –æ—Ç–≤–µ—Ç.
    """
    try:
        if not update.message or not update.message.text:
            return
        chat_id = update.message.chat_id
        text = update.message.text
        print(f"[–õ–û–ì] –ù–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –∏–∑ —á–∞—Ç–∞ {chat_id}, —Ç–µ–∫—Å—Ç: {text!r}")

        # 1) –ú–æ–¥–µ—Ä–∞—Ü–∏—è/—Å–ø–∞–º/—Å—Å—ã–ª–∫–∏/—É–ø–æ–º–∏–Ω–∞–Ω–∏—è
        if _is_violation(text):
            try:
                await context.bot.delete_message(
                    chat_id=chat_id, message_id=update.message.message_id
                )
            except Exception:
                return
            warning = await update.message.reply_text(WARNING_TEXT)
            asyncio.create_task(
                _delete_warning_later(context, chat_id, warning.message_id)
            )
            return

        # 2) –ö–æ–º–∞–Ω–¥–∞ /clear (—Ç–æ–ª—å–∫–æ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä)
        if text.strip().lower().startswith("/clear"):
            user_id = update.message.from_user.id if update.message.from_user else None
            if user_id is None or not await _is_admin(context, chat_id, user_id):
                return
            try:
                await context.bot.delete_message(
                    chat_id=chat_id, message_id=update.message.message_id
                )
            except Exception:
                pass
            # –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 100 —Å–æ–æ–±—â–µ–Ω–∏–π, –Ω–∞—á–∏–Ω–∞—è —Å —Ç–µ–∫—É—â–µ–≥–æ
            current_id = update.message.message_id
            for msg_id in range(current_id, max(current_id - 100, 0), -1):
                try:
                    print(f"DEBUG: –£–¥–∞–ª—è—é —Å–æ–æ–±—â–µ–Ω–∏–µ ID={msg_id}")
                    await context.bot.delete_message(chat_id=chat_id, message_id=msg_id)
                except Exception:
                    continue
                await asyncio.sleep(0.1)
            notice = await context.bot.send_message(
                chat_id=chat_id,
                text="üßπ –ß–∞—Ç –æ—á–∏—â–µ–Ω (–Ω–∞—Å–∫–æ–ª—å–∫–æ –ø–æ–∑–≤–æ–ª–∏–ª–∏ –ª–∏–º–∏—Ç—ã Telegram)",
            )
            asyncio.create_task(_delete_notice_later(context, chat_id, notice.message_id))
            return

        # 3) –ü–æ–∏—Å–∫ –ø–æ –±–∞–∑–µ
        reply_text = get_peptide_info(text)
        await update.message.reply_text(reply_text, parse_mode="Markdown")
    except Exception as e:
        print(f"[–û–®–ò–ë–ö–ê] –ü—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–æ–æ–±—â–µ–Ω–∏—è: {e}")
    finally:
        print("DEBUG: –ë–æ—Ç –≥–æ—Ç–æ–≤ –∫ —Å–ª–µ–¥—É—é—â–µ–º—É –∑–∞–ø—Ä–æ—Å—É")

def run_polling() -> None:
    token = os.getenv("ARBITER_TOKEN")
    if not token:
        print("Missing ARBITER_TOKEN in .env")
        sys.exit(1)

    print(f"–ü—É—Ç—å –∫ –±–∞–∑–µ: {DB_PATH}")
    app = ApplicationBuilder().token(token).build()
    app.add_handler(MessageHandler(filters.TEXT, _handle_update))
    app.run_polling(drop_pending_updates=True)

if __name__ == "__main__":
    run_polling()




